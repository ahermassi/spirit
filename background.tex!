% (C) 2016 Jean Nassar. Some Rights Reserved
% Except where otherwise noted, this work is licensed under the Creative Commons Attribution-ShareAlike License, version 4
% Background
\chapter{Background and Proposal}
\label{ch:background}

\section{Prior work}
  Several methods to show a robotic avatar in a virtual environment were proposed.

  Nielsen et al. presented an interface in which a \gls{not:3d} model of the robot, a video stream from a mounted camera, a map, and pose information, were combined in a mixed-reality display.\cite{nielsen2007}
  Kelly et al. extended the concept to an outdoor environment, and succeeded in making their system work in various low-throughput situations.\cite{kelly2011}

  However, most methods require many sensors, which add expense and weight to the vehicle.

\section{Teleoperation System Using Past Image Records}
  Shiroma et al. introduced \gls{spir}, a past-image view technique for mobile robots.\cite{shiroma2004}
  This technology enabled the operator to ascertain the position and orientation of the robot with respect to its environment from a third-person (bird's-eye view) perspective using only the camera and position and pose information.

  \begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
      \includegraphics[width=\textwidth]{spir_system_overview}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
      \includegraphics[width=\textwidth]{spir_birdseye}
    \end{subfigure}
    \caption{System overview of Past-Image View.\cite{shiroma2004}}
    \label{fig:spir_system_overview}
  \end{figure}

  Images taken by a raised camera were saved in a buffer along with their position and the time at which they were taken.
  No images are stored while the robot is not moving.
  Three evaluation functions were proposed to select an optimal image to use as the background:

  \begin{description}
    \item [Fixed time delay:] uses images taken a certain period earlier.
    \item [Fixed distance:] uses images taken at approximately the given distance.
    \item [\Gls{fov} evaluation:] selects the closest image if the robot is in the field of view.
  \end{description}

  \begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
      \includegraphics[width=\textwidth]{spir_time_delay}
      \caption{Fixed time delay algorithm.}
      \label{fig:spir_time_delay}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
      \includegraphics[width=\textwidth]{spir_fixed_distance}
      \caption{Fixed distance algorithm.}
      \label{fig:spir_fixed_distance}
    \end{subfigure}
    \caption{The na√Øve algorithms.\cite{shiroma2004}}
    \label{fig:spir_naive_algorithms}
  \end{figure}

  \Gls{fov} evaluation uses the fixed-distance algorithm as the base, but also calculates view and centreline distances, as well as the widths at the bottom and centre of the image.

  \begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
      \includegraphics[width=\textwidth]{spir_fov_side_view}
      \caption{Side view.}
      \label{fig:spir_fov_side_view}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
      \includegraphics[width=\textwidth]{spir_fov_top_view}
      \caption{Top view.}
      \label{fig:spir_fov_top_view}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
      \includegraphics[width=\textwidth]{spir_fov}
      \caption{\Gls{fov} evaluation.}
      \label{fig:spir_fov_flowchart}
    \end{subfigure}
    \caption{\Gls{fov} evaluation algorithm.\cite{shiroma2004}}
    \label{fig:spir_fov}
  \end{figure}

  An optimal image was then found using an evaluation function considering several constraints, including \gls{fov} and the relative positions of the camera and robot.
  The latest image is used if no good solution can be found.

  If the bandwidth is low, old images can be reused, updated by an array of position data sent at a higher speed than the video.

  \subsection{Further development}
    Sugimoto et al. demonstrated the effectiveness of \gls{spir} with user experiments.\cite{sugimoto2005}
    Ito et al. decreased choppiness by proposing wider \glspl{fov}, and zooming into the background and model as the robot went further into the image plane.\cite{ito2008}
    They also experimented with adding trajectory forecasting with unmanned ground vehicles.

    \begin{figure}[h]
      \centering
      \begin{subfigure}[b]{0.45\textwidth}
	\includegraphics[width=\textwidth]{zoom_live}
      \end{subfigure}
      \hfill
      \begin{subfigure}[b]{0.45\textwidth}
	\includegraphics[width=\textwidth]{zoom_combined}
      \end{subfigure}
      \caption{Zoom with bird's-eye view synthesis.\cite{ito2008}}
      \label{fig:zoom_results}
    \end{figure}

    Murata et al. extended the concept to \gls{not:3d} when they applied it to mobile manipulators.\cite{murata2014}
    They showed a significant decrease in operators' position error in the vertical direction with a similar error in the longitudinal direction.

    \begin{figure}[h]
      \centering
      \begin{subfigure}[b]{0.45\textwidth}
	\includegraphics[width=\textwidth]{manip_overview}
	\caption{Geometric relationships.}
      \end{subfigure}
      \hfill
      \begin{subfigure}[b]{0.45\textwidth}
	\includegraphics[width=\textwidth]{manip_result}
	\caption{Basic display concept.}
      \end{subfigure}
      \caption{\gls{spir} for a mobile manipulator.\cite{murata2014}}
      \label{fig:manip_results}
    \end{figure}

\section{Problems of applying SPIR to quadrotors}
  When a quadrotor flies forwards, the craft pitches down.
  For cameras which do not offer gimballing or a wide \gls{fov}, though, this means that the horizon could go out of frame.
  As a result, older video would need to be used, which would likely be from a longer distance.
  If the drone turns while going forward, the only usable images might be the live ones.

  One potential mitigating factor would be to set a maximum pitch angle, though that would limit the speed of the drone.
  Even so, during straight and level flight, an operator would only be able to see the drone from behind, unless an elevated camera is attached.

  Another possible solution would be to tilt the camera upwards.
  This would limit the amount of ground it would see during a hover.
  In addition, it would cause the camera to be pointing at the sky while the drone is outdoors and moving backwards, removing potential reference points.

  Finally, localization can be difficult too.
  Even outdoors, \gls{gps} only tends to be accurate to within a few metres, which is much larger than the widths of most drones.
  While estimates of acceleration can be obtained by modelling the motor outputs and using built-in accelerometers and gyroscopes, there is often drift from wind, as well as significant vibration even during hovering.
  Integrating acceleration twice to obtain position would cause exponentially increasing drift.

  External sources could be used to reduce the error, such as motion capture cameras if indoors, binocular cameras, visual odometry, \gls{gps}, or \gls{slam}.
  That might require the use of more additional tools, such as ultrasonic distance sensors or laser range finders.
  An \gls{ekf} could then be used to estimate the best location and minimize drift.
  However, that is beyond the scope of this research.

  Some \gls{slam} algorithms, such as \gls{ptam} and \gls{rtslam}, are able to use monocular cameras, but rotation on the spot often causes them to lose keypoints, making them a bad choice for drones.

\section{Proposal}
  The proposal was to upgrade \gls{spir} to allow it to work better with quadrotors.
  Operator workload should decrease in positioning and orientation tasks.

  A framework should be built which would allow the modular implementation of the core functions of \gls{spir}:

  \begin{enumerate}
    \item Obtain the robot position by the best method for each robot.
    \item Save images obtained by the camera, and the pose and time at which they were taken.
    \item Select the best frame according to an evaluation function.
          That robot should ideally be seen from a good position and orientation with respect to the rest of the environment.
    \item Generate and display the rendering of the current robot position onto the selected frame.
  \end{enumerate}

  The system was built in \gls{ros} for interoperability and future expansion, as well as its ease of modularization.
